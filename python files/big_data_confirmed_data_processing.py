# -*- coding: utf-8 -*-
"""big_data_confirmed_data_processing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GV7aLSbrQOLQ7WpNaTibeUhp9jZz_aR4
"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from numpy import inf

# !pip install plotly
import plotly.express as px

# !pip install pyramid-arima
from pyramid.arima import auto_arima
from statsmodels.tsa.arima_model import ARIMA
from statsmodels.graphics.tsaplots import plot_acf,plot_pacf

from datetime import datetime, timedelta,date
import matplotlib.pyplot as plt
from matplotlib import ticker 

from keras.layers import Input, Dense, Activation, LeakyReLU
from keras import models
from keras.optimizers import RMSprop, Adam
from keras.utils import plot_model

from datetime import datetime, timedelta,date
import numpy as np

df_confirmed = pd.read_csv('https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_US.csv')
df_confirmed.head()
# df_confirmed.head()
columns_to_be_removed = ['UID', 'iso2', 'iso3', 'code3', 'FIPS', 'Admin2', 'Lat', 'Long_', 'Combined_Key', 'Country_Region']
df_confirmed = df_confirmed.drop(columns_to_be_removed, axis= 1)
df_confirmed = df_confirmed.replace(np.nan, '', regex=True)
df_confirmed = df_confirmed.groupby(['Province_State']).sum()
df_confirmed = df_confirmed.T

states = ["New York", "Michigan", "California", "Kentucky", "Texas"]

for state in states:
  state_data = df_confirmed[state]
  df_state = pd.DataFrame(state_data)
  df_state.plot()
  plt.title("Confirmed Cases")
  plt.show()

#defining a four layer deep learning model in keras with tensorflow as backend.
#The first layer has 80 nodes, 
def model_init():
  Visible = Input(shape=(1,))
  Dense_l1 = Dense(80,name="Dense_l1")(Visible)
  LRelu_l1 = LeakyReLU(name = "LRelu_l1")(Dense_l1)
  Dense_l2 = Dense(80,name = "Dense_l2")(LRelu_l1)
  LRelu_l2 = LeakyReLU(name = "LRelu_l2")(Dense_l2)
  Dense_l3 = Dense(100,name="Dense_l3")(LRelu_l2)
  LRelu_l3 = LeakyReLU(name = "LRelu_l3")(Dense_l3)
  Dense_l4 = Dense(1,name="Dense_l4")(LRelu_l3)
  LRelu_l4 = LeakyReLU(name = "Output")(Dense_l4)
  model_four = models.Model(inputs=Visible, outputs=LRelu_l4)
  model_four.compile(optimizer=Adam(lr=0.001), 
                loss='mean_squared_error',
                metrics=['accuracy'])
  model_four.summary()
  return model_four

model = model_init()
plot_model(model)

#preparing the model for top 5 states

# for state in states:
#   data_y = np.log10(np.asarray(df_confirmed[state]).astype("float32"))
#   data_y[data_y == -inf] = 0
#   data_x = np.arange(1,len(data_y)+1)

#   epochs = 5000
#   model = model_init()
#   model.fit(data_x.reshape([data_y.shape[0],1]),data_y.reshape([data_y.shape[0],1]),epochs=epochs)
#   #saving the model with state name extension
#   model.save("models/model_"+state+".h5")

#preparing the date array from Jan 22nd
date_array = [datetime.strptime(date,'%m/%d/%y').strftime("%d %b") for date in df_confirmed.index]
thousand = 1000
no_of_predictions = 7

#adding next 7 days
dates = []
predictionDays = [(datetime.strptime(date_array[-1],'%d %b')+timedelta(days=i)).strftime("%d %b") for i in range(1,no_of_predictions+1)]
for i in range(no_of_predictions):
   dates.append(predictionDays[i] + " 2020")
prediction_df = pd.DataFrame(dates, columns=["Dates"])

#fitting the model for each state and predicting the confirmed cases
for state in states:
  #preparing data for confirmed states, state wise
  #taking log of data to reduce mean sqaures error value
  data_y = np.log10(np.asarray(df_confirmed[state]).astype("float32"))
  data_y[data_y == -inf] = 0
  data_x = np.arange(1,len(data_y)+1)

  #retriving the saved model for the states and fitting the data into it
  model = models.load_model("models/model_" + state + ".h5")
  data = np.power(10,model.predict(np.arange(1,len(data_y)+no_of_predictions+1)))

  temp_data = df_confirmed[state]
  temp_data[temp_data == 0] = 1

  #plotting the prediction curve vs actual curve
  f = plt.figure(figsize=(15,10))
  ax = f.add_subplot(111)
  date = np.arange(0,len(temp_data))
  marker_style = dict(linewidth=3, linestyle='-', marker='o',markersize=7, markerfacecolor='#ffffff')
  plt.plot(date,temp_data/thousand,"-.",color="orange",**marker_style, label="Actual Curve")

  date = np.arange(0,len(data))
  plt.plot(date,data/thousand,"-.",color="green",label="Predicted Curve")

  # nextdays = [(datetime.strptime(d[-1],'%d %b')+timedelta(days=i)).strftime("%d %b") for i in range(1,no_of_predictions+1)]
  total = date_array + nextdays

  nextDaysPrediction = []
  for i in range(no_of_predictions):
     nextDaysPrediction.append(str(np.round(data[-1*(no_of_predictions-i)],-3)[0]/thousand))

  plt.title("Prediction Curve for Confirmed Cases for " + state ,{'fontsize':24})

  plt.xticks(list(np.arange(0,len(total),int(len(total)/5))),d[:-1:int(len(total)/5)]+[total[-1]])

  ax.xaxis.set_minor_locator(ticker.AutoMinorLocator())
  ax.yaxis.set_minor_locator(ticker.AutoMinorLocator())
  ax.tick_params(which='both', width=1,labelsize=14)
  ax.tick_params(which='major', length=6)
  ax.tick_params(which='minor', length=3, color='0.8')

  plt.xlabel("Date",fontsize =20)
  plt.ylabel("Number of Confirmed Cases",fontsize =20)

  plt.yscale("log")
  plt.legend(fontsize = 18)
  plt.tick_params(labelsize = 13) 

  plt.show()

  prediction_df[state + " in thousands"] = nextDaysPrediction

#final predictions for 5 states
prediction_df