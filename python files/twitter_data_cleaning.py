# -*- coding: utf-8 -*-
"""twitter_data_cleaning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hum2Y8cVh8xcUQx1umr3KcLziFz5L6xa
"""

# !pip install pyspark

# !apt-get install openjdk-8-jdk-headless -qq > /dev/null
# !pip install pyspark td-pyspark

import pyspark
import json

import re
import string
import csv
import datetime

import os
def htags(ht):# getting Hashtags in a tweet
  y = ht.replace("#","")
  z= y.split(' ')
  return (z)

def getHashtags(htags):# getting Hashtags in a tweet
  hash_list=[]
  if htags != '[]' and htags:
    y= htags[1:len(htags)-1]
    z= y.replace("\'",'\"')
    at= eval(z)
    if type(at) in [dict]:
      at= tuple(at)
    for hashes in at:
      hash_list.append(hashes["text"])
    return hash_list
  else: return hash_list



def preprocess_tweet(text): #cleaning tweets
    # remove usernames
    nopunc = re.sub('@[^\s]+', '', text)
    # Check characters to see if they are in punctuation
    nopunc = [char for char in nopunc if char not in string.punctuation]
    # Join the characters again to form the string.
    nopunc = ''.join(nopunc)
    # convert text to lower-case
    nopunc = nopunc.lower()
    # remove URLs
    nopunc = re.sub('((www\.[^\s]+)|(https?://[^\s]+)|(http?://[^\s]+))', '', nopunc)
    nopunc = re.sub(r'http\S+', '', nopunc)
    # remove the # in #hashtag
    nopunc = re.sub(r'#([^\s]+)', r'\1', nopunc)
    #replacing the non ascii chars with space
    nopunc = re.sub(r'[^\x00-\x7f]',r' ',nopunc)
    #checkinif string starts with 'rt '
    if(nopunc.startswith("rt ")):
      nopunc = nopunc[3:]
    #removing multiple spaces
    nopunc = re.sub(' +', ' ', nopunc)
    #stripping whitespace
    nopunc = nopunc.strip()
    return nopunc

def getText_Loc_Hashtags(rdd): # formatting the csv data in {"text": text, "Geo":geo}
  text = preprocess_tweet(rdd[3])
  hashtaglist = htags(rdd[6])
  for hashtag in hashtaglist:
    text += " " + hashtag
  return {"text": text, "geo":rdd[4]}


sc = pyspark.SparkContext.getOrCreate()
folders = ["may1", "april1", "april2"]
for folder in folders: #reading all the dataset
    for file in os.listdir("/content/new/{}".format(folder)):
        if file.endswith(".csv"):
            fullpath = os.path.join("/content/new/{}".format(folder), file)
            fullpath1 = os.path.join("/content/new1/{}".format(folder), file)
            reader1 = csv.reader(open(fullpath, 'r', encoding='utf8'))
            rdd2 = sc.parallelize(reader1)
            rdd= rdd2.map(lambda x: None if x[0]=='' else x).filter(bool) # removing the header row
            rdds = rdd.map(getText_Loc_Hashtags)
            rdds.saveAsTextFile(fullpath1)

sc = pyspark.SparkContext.getOrCreate()
reader1 = csv.reader(open('/content/new/may2/2020-05-07.csv', 'r', encoding='utf8'))

rdd2 = sc.parallelize(reader1)
rdd= rdd2.map(lambda x: None if x[0]=='' else x).filter(bool)
rdd.take(6)

import pyspark
import json

import re
import string
import csv
import datetime

import os

def convert_to_tuple(at):
    if type(at) == dict:
        temp=[at]
        return(tuple(temp))

    else:
        return at
def getHashtags(htags): # getting Hashtags in a tweet
  hash_list=[]
  if htags != '[]' and htags:
    y= htags[1:len(htags)-1]
    z= y.replace("\'",'\"')
    at= eval(z)
    at= convert_to_tuple(at)
    for hashes in at:
      hash_list.append(hashes["text"])
    return hash_list
  else: return hash_list

def preprocess_tweet(text): #cleaning tweets
    nopunc = re.sub('@[^\s]+', '', text)
    # Check characters to see if they are in punctuation
    nopunc = [char for char in nopunc if char not in string.punctuation]
    # Join the characters again to form the string.
    nopunc = ''.join(nopunc)
    # convert text to lower-case
    nopunc = nopunc.lower()
    # remove URLs
    nopunc = re.sub('((www\.[^\s]+)|(https?://[^\s]+)|(http?://[^\s]+))', '', nopunc)
    nopunc = re.sub(r'http\S+', '', nopunc)
    # remove the # in #hashtag
    nopunc = re.sub(r'#([^\s]+)', r'\1', nopunc)
    #replacing the non ascii chars with space
    nopunc = re.sub(r'[^\x00-\x7f]',r' ',nopunc)
    #checkinif string starts with 'rt '
    if(nopunc.startswith("rt ")):
      nopunc = nopunc[3:]
    #removing multiple spaces
    nopunc = re.sub(' +', ' ', nopunc)
    #stripping whitespace
    nopunc = nopunc.strip()
    return nopunc

def getText_Loc_Hashtags(rdd): # formatting the csv data in {"text": text, "Geo":geo}
  text = preprocess_tweet(rdd[5])
  hashtaglist = getHashtags(rdd[7])
  for hashtag in hashtaglist:
    text += " " + hashtag
  return {"text": str(text), "geo":str(rdd[6])}


sc = pyspark.SparkContext.getOrCreate()
folders = ["may2"]
for folder in folders: #reading all the dataset
    for file in os.listdir("/content/new/{}".format(folder)):
        if file.endswith(".csv"):
            fullpath = os.path.join("/content/new/{}".format(folder), file)
            fullpath1 = os.path.join("/content/new1/{}".format(folder), file)
            reader1 = csv.reader(open(fullpath, 'r', encoding='utf8'))
            rdd2 = sc.parallelize(reader1)
            rdd= rdd2.map(lambda x: None if x[0]=='' else x).filter(bool) # removing the header row
            rdds = rdd.map(getText_Loc_Hashtags)
            rdds.saveAsTextFile(fullpath1)

rdds.saveAsTextFile("sample_data/data_process_new")

# we collate all the RDDs from months January to May into single RDD which is further sent to tensorflow for analysis.
folders = ["may2", "may1", "april1", "april2", "tweets_old"]
final_rdd=[]
f = ""
for folder in folders:
    for file in os.listdir("/content/new1/{}".format(folder)):
      f += '/content/new1/{}/'.format(folder)+file+","
cs_files = f[0:-1]
rdd_full = sc.textFile(cs_files)
rd = rdd_full.map(lambda x: eval(x))

rd.saveAsTextFile('/content/final_april_may')